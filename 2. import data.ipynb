{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3",
   "language": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "^C\n"
     ]
    }
   ],
   "source": [
    "!pip install --upgrade azureml-sdk azureml-widgets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Ready to use Azure ML 1.19.0 to work with dp100-WS\n"
     ]
    }
   ],
   "source": [
    "import azureml.core\n",
    "from azureml.core import Workspace\n",
    "from azureml.core import Dataset\n",
    "\n",
    "# Load the workspace from the saved config file\n",
    "ws = Workspace.from_config('config.json')\n",
    "print('Ready to use Azure ML {} to work with {}'.format(azureml.core.VERSION, ws.name))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "azureml_globaldatasets - Default = False\nworkspaceblobstore - Default = True\nworkspacefilestore - Default = False\n"
     ]
    }
   ],
   "source": [
    "# Get the default datastore\n",
    "default_ds = ws.get_default_datastore()\n",
    "\n",
    "# Enumerate all datastores, indicating which is the default\n",
    "for ds_name in ws.datastores:\n",
    "    print(ds_name, \"- Default =\", ds_name == default_ds.name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Datasets registered\n"
     ]
    }
   ],
   "source": [
    "# upload file to default datastore.  Note that you will need to run this from local server if the files are local\n",
    "def upload(input_folder_path, file_name):\n",
    "    #Upload file\n",
    "    default_ds.upload_files(files=[input_folder_path+file_name], # Upload\n",
    "                       target_path=output_path, # Put it in a folder path in the datastore\n",
    "                       overwrite=True, # Replace existing files of the same name\n",
    "                       show_progress=True)\n",
    "\n",
    "# transform the file into a tabular data set\n",
    "def create_tab_dataset(output_path, file_name):\n",
    "    #Create a tabular dataset from the path on the datastore (this may take a short while)\n",
    "    tab_data_set = Dataset.Tabular.from_delimited_files(path=(default_ds, output_path+file_name))\n",
    "\n",
    "    # Display the first 20 rows as a Pandas dataframe\n",
    "    tab_data_set.take(20).to_pandas_dataframe()\n",
    "\n",
    "    return tab_data_set\n",
    "\n",
    "# register the data set in the workspace for easy consumption\n",
    "def register_tab_dataset(tab_data_set, name):\n",
    "    # Register the tabular dataset\n",
    "    try:\n",
    "        tab_data_set = tab_data_set.register(workspace=ws, \n",
    "                                            name=name,\n",
    "                                            description='',\n",
    "                                            tags = {'format':'CSV'},\n",
    "                                            create_new_version=True)\n",
    "    except Exception as ex:\n",
    "        print(ex)\n",
    "    print('Datasets registered')\n",
    "\n",
    "# upload and register data set\n",
    "def upload_and_register_tabular_dataset(input_folder_path, file_name, output_path, name):\n",
    "    upload(input_folder_path, file_name)\n",
    "    tab_data_set = create_tab_dataset(output_path, file_name)\n",
    "    register_tab_dataset(tab_data_set, name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "upload_and_register_tabular_dataset('C:/Users/kelvi/OneDrive/Documents/Data Projects/Data/', 'bureau_balance.csv', 'credit data/', 'bureau balance')\n",
    "#unknown error when trying with some files.  Only thing I can see that might be the issue is that it happens with large files.  Seems ok with 160MB but anything larger and it fails.  Might be something else though\n",
    "#workaround just now is to use the azure storage explorer to upload all the files and then register the data sets using these functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tab_data_set = create_tab_dataset('credit data/', 'creditcard.csv')\n",
    "register_tab_dataset(tab_data_set, 'credit card')"
   ]
  }
 ]
}